import { useState, useEffect, useRef } from 'react';
import './App.css';

const WS_URL = (() => {
  const url = import.meta.env.VITE_WS_URL;
  // import.meta.env.VITE_WS_URL ;
  // Convert HTTP/HTTPS to WS/WSS for WebSocket connections
  if (url.startsWith('https://')) {
    return url.replace('https://', 'wss://');
  } else if (url.startsWith('http://')) {
    return url.replace('http://', 'ws://');
  }
  return url; // Already ws:// or wss://
})();

function App() {
  const [isConnected, setIsConnected] = useState(false);
  const [isCallActive, setIsCallActive] = useState(false);
  const [status, setStatus] = useState('idle'); // idle, listening, processing, speaking
  const [error, setError] = useState(null);

  const wsRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const audioContextRef = useRef(null);
  const audioQueueRef = useRef([]);
  const isPlayingRef = useRef(false);
  const audioBufferRef = useRef(null);
  const isCallActiveRef = useRef(false); // Use ref to track call state in audio callback
  const recognitionRef = useRef(null); // Web Speech API recognition
  const useWebSpeechAPI = true; // Set to true to use free Web Speech API instead of Deepgram
  const ttsQueueRef = useRef([]); // Queue for TTS utterances
  const isSpeakingRef = useRef(false); // Track if currently speaking
  const pcmBufferRef = useRef(new Uint8Array(0)); // Buffer for accumulating PCM chunks
  const lastChunkTimeRef = useRef(Date.now()); // Track last chunk arrival time for network monitoring
  const networkMonitorRef = useRef(null); // Network monitoring interval
  const silenceTimerRef = useRef(null); // Timer for Turbo VAD
  const lastSpeechTimeRef = useRef(0); // Timestamp of last interim speech
  const interimTranscriptRef = useRef(''); // Buffer for interim transcript

  useEffect(() => {
    return () => {
      // Cleanup on unmount
      // Stop network monitoring
      clearInterval(networkMonitorRef.current);
      networkMonitorRef.current = null;

      // Stop Turbo VAD timer
      if (silenceTimerRef.current) {
        clearInterval(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }

      if (wsRef.current) {
        try {
          wsRef.current.close();
        } catch (e) {
          // Ignore cleanup errors
        }
      }

      // Cleanup media recorder (our custom object with stream, processor, source)
      if (mediaRecorderRef.current) {
        try {
          if (mediaRecorderRef.current.stream) {
            mediaRecorderRef.current.stream.getTracks().forEach((track) => track.stop());
          }
          if (mediaRecorderRef.current.processor) {
            mediaRecorderRef.current.processor.disconnect();
          }
          if (mediaRecorderRef.current.source) {
            mediaRecorderRef.current.source.disconnect();
          }
        } catch (e) {
          // Ignore cleanup errors
        }
        mediaRecorderRef.current = null;
      }

      if (audioContextRef.current) {
        try {
          audioContextRef.current.close();
        } catch (e) {
          // Ignore cleanup errors
        }
      }
    };
  }, []);

  // Monitor recognition state and ensure it stays active during call
  useEffect(() => {
    if (!isCallActive || !useWebSpeechAPI) {
      return;
    }

    // More aggressive periodic check to ensure recognition is running
    const recognitionCheckInterval = setInterval(() => {
      // Don't restart if speaking or playing audio (prevent echo)
      if (isCallActiveRef.current && recognitionRef.current && !isSpeakingRef.current && !isPlayingRef.current) {
        // Try to restart recognition if it's not running
        try {
          recognitionRef.current.start();
        } catch (e) {
          const errorMsg = e.message || String(e);
          // If it's already running, that's fine - ignore the error
          if (!errorMsg.includes('already') && !errorMsg.includes('started')) {
            console.log('üîÑ Recognition check: attempting restart...');
            restartRecognition();
          }
        }
      }
    }, 1000); // Check every 1 second (very frequent)

    return () => {
      clearInterval(recognitionCheckInterval);
    };
  }, [isCallActive, useWebSpeechAPI]);

  const connectWebSocket = () => {
    try {
      const ws = new WebSocket(WS_URL);
      ws.binaryType = 'arraybuffer'; // ensure we get ArrayBuffer for audio
      wsRef.current = ws;

      ws.onopen = () => {
        console.log('WebSocket connected');
        setIsConnected(true);
        setError(null);

        // Start network monitoring to detect connection issues
        if (networkMonitorRef.current) {
          clearInterval(networkMonitorRef.current);
        }
        lastChunkTimeRef.current = Date.now(); // Reset timer
        networkMonitorRef.current = setInterval(() => {
          const timeSinceLastChunk = Date.now() - lastChunkTimeRef.current;
          // If no chunks received for 5 seconds during active call, there might be a network issue
          if (isCallActive && timeSinceLastChunk > 5000 && isPlayingRef.current) {
            console.warn('‚ö†Ô∏è No audio chunks received for 5 seconds - possible network issue');
          }
        }, 2000); // Check every 2 seconds
      };

      ws.onmessage = (event) => {
        // Handle text messages (TTS text or errors)
        if (typeof event.data === 'string') {
          try {
            const data = JSON.parse(event.data);
            if (data.type === 'tts_text' && data.text) {
              console.log('üìù Received TTS text (fallback to Web Speech API):', data.text);
              // Use Web Speech API for TTS (free, browser-based)
              speakText(data.text);
            } else if (data.error) {
              setError(data.error);
            }
          } catch (e) {
            // Not JSON, ignore
          }
        } else if (event.data instanceof ArrayBuffer || event.data instanceof Blob) {
          // Handle audio chunks from ElevenLabs
          // Reduced logging to prevent performance issues
          handleAudioChunk(event.data);
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        // Don't immediately show error - try to reconnect
        if (!isCallActive) {
          setError('Connection error. Please ensure the backend is running on port 8080.');
        }
        setIsConnected(false);
      };

      ws.onclose = (event) => {
        console.log('WebSocket disconnected', event.code, event.reason?.toString());
        setIsConnected(false);

        // Stop network monitoring
        if (networkMonitorRef.current) {
          clearInterval(networkMonitorRef.current);
          networkMonitorRef.current = null;
        }

        if (isCallActive) {
          stopCall();
        }
        // Show error with details if it wasn't a clean close
        // 1000 = Normal closure, 1001 = Going away, 1005 = No status received
        if (event.code !== 1000 && event.code !== 1001) {
          const reason = event.reason || 'Unknown reason';
          if (event.code === 1005) {
            // No status received - connection closed without proper handshake
            // This is usually fine if user initiated the close
            console.log('Connection closed without status (likely user-initiated)');
            // Don't show error for 1005 if call was already stopped
            if (isCallActive) {
              setError('Connection closed unexpectedly. Please try again.');
            }
          } else if (event.code === 1006) {
            setError('Connection closed abnormally. The backend may have encountered an error (check backend logs). This often happens if Deepgram API key is invalid or there\'s a configuration issue.');
          } else {
            setError(`Connection lost (code: ${event.code}, reason: ${reason}). Please try again.`);
          }
        }
      };
    } catch (error) {
      console.error('Failed to connect:', error);
      setError('Failed to connect to server. Please check the backend URL.');
    }
  };

  // Web Speech API TTS function (free, browser-based) with queue
  const speakText = (text) => {
    if (!text || text.trim().length === 0) return;

    console.log('üîä Queueing text for TTS:', text);

    // Add to queue
    ttsQueueRef.current.push(text);

    // Start processing queue if not already speaking
    if (!isSpeakingRef.current) {
      processTTSQueue();
    }
  };

  // Helper function to restart recognition robustly
  const restartRecognition = () => {
    if (!isCallActiveRef.current || !recognitionRef.current) {
      return;
    }

    // Check if recognition is already running by checking its state
    // Note: Web Speech API doesn't expose a direct state property, so we use try-catch
    try {
      recognitionRef.current.start();
      console.log('‚úÖ Web Speech API recognition started');
    } catch (e) {
      const errorMsg = e.message || String(e);
      if (errorMsg.includes('already') || errorMsg.includes('started') || errorMsg.includes('aborted')) {
        console.log('‚ÑπÔ∏è Recognition already running or starting');
        // If it's already running, that's fine - but ensure it stays running
        // Sometimes recognition stops silently, so we'll try again after a short delay
        setTimeout(() => {
          if (isCallActiveRef.current && recognitionRef.current) {
            try {
              recognitionRef.current.start();
            } catch (e2) {
              // Ignore "already started" errors
              const msg2 = e2.message || String(e2);
              if (!msg2.includes('already') && !msg2.includes('started')) {
                console.warn('‚ö†Ô∏è Recognition restart warning:', msg2);
              }
            }
          }
        }, 200);
      } else {
        console.log('‚ö†Ô∏è Recognition error, will retry:', errorMsg);
        // Retry after a delay
        setTimeout(() => {
          if (isCallActiveRef.current && recognitionRef.current) {
            try {
              recognitionRef.current.start();
              console.log('‚úÖ Web Speech API recognition restarted (retry)');
            } catch (e2) {
              console.error('‚ùå Failed to restart recognition:', e2.message);
              // Last resort: try one more time after a longer delay
              setTimeout(() => {
                if (isCallActiveRef.current && recognitionRef.current) {
                  try {
                    recognitionRef.current.start();
                    console.log('‚úÖ Web Speech API recognition restarted (final retry)');
                  } catch (e3) {
                    console.error('‚ùå Final retry failed:', e3.message);
                  }
                }
              }, 300);
            }
          }
        }, 200);
      }
    }
  };

  const processTTSQueue = () => {
    if (ttsQueueRef.current.length === 0) {
      isSpeakingRef.current = false;
      setStatus(isCallActive ? 'listening' : 'idle');

      // After all speech is done, ensure recognition is still running
      // Use a longer delay to ensure audio playback has fully completed
      setTimeout(() => {
        restartRecognition();
      }, 800); // Increased delay to ensure speech is fully complete
      return;
    }

    if (!('speechSynthesis' in window)) {
      console.warn('‚ö†Ô∏è SpeechSynthesis not supported in this browser');
      ttsQueueRef.current = [];
      isSpeakingRef.current = false;
      setStatus(isCallActive ? 'listening' : 'idle');
      return;
    }

    isSpeakingRef.current = true;
    setStatus('speaking');

    const text = ttsQueueRef.current.shift();
    console.log('üîä Speaking text:', text);

    const utterance = new SpeechSynthesisUtterance(text);

    // Try to find an Indian English voice, fallback to default
    const voices = window.speechSynthesis.getVoices();
    const indianVoice = voices.find(voice =>
      voice.lang.includes('en') && (
        voice.name.toLowerCase().includes('india') ||
        voice.name.toLowerCase().includes('neha') ||
        voice.name.toLowerCase().includes('priya') ||
        voice.name.toLowerCase().includes('rishi')
      )
    ) || voices.find(voice => voice.lang.startsWith('en'));

    if (indianVoice) {
      utterance.voice = indianVoice;
      console.log('üéôÔ∏è Using voice:', indianVoice.name);
    }

    // Speed up speech a bit to make responses feel more snappy
    utterance.rate = 1.25; // Increase if you want it faster (max ~2.0)
    utterance.pitch = 1.0;
    utterance.volume = 1.0;

    utterance.onend = () => {
      console.log('‚úÖ Speech completed');
      // Process next item in queue (will handle recognition restart when queue is empty)
      processTTSQueue();
    };

    utterance.onerror = (error) => {
      // "interrupted" is expected when new speech starts, ignore it
      if (error.error === 'interrupted') {
        console.log('‚ÑπÔ∏è Speech interrupted (expected when new text arrives)');
      } else {
        console.error('‚ùå Speech synthesis error:', error.error);
      }
      // Process next item in queue even on error
      processTTSQueue();
    };

    window.speechSynthesis.speak(utterance);
  };

  const handleAudioChunk = async (audioData) => {
    // Update last chunk time for network monitoring
    lastChunkTimeRef.current = Date.now();

    console.log('üîä Received audio chunk, size:', audioData.byteLength || audioData.size || 'unknown');

    // Ensure audio context is initialized and resumed
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000,
      });
    }

    const audioContext = audioContextRef.current;

    // Resume audio context if suspended (browser autoplay policy)
    if (audioContext.state === 'suspended') {
      try {
        await audioContext.resume();
        console.log('‚úÖ Audio context resumed');
      } catch (error) {
        console.error('‚ùå Failed to resume audio context:', error);
      }
    }

    // Convert Blob to ArrayBuffer if needed
    let arrayBuffer = audioData instanceof Blob
      ? await audioData.arrayBuffer()
      : audioData instanceof ArrayBuffer
        ? audioData
        : audioData.buffer;

    // Handle raw PCM audio (16-bit, 16kHz)
    const newData = new Uint8Array(arrayBuffer);
    const combined = new Uint8Array(pcmBufferRef.current.length + newData.length);
    combined.set(pcmBufferRef.current);
    combined.set(newData, pcmBufferRef.current.length);
    pcmBufferRef.current = combined;

    // Process when we have at least 512 bytes for minimal latency
    const MIN_CHUNK_SIZE = 512;

    while (pcmBufferRef.current.length >= MIN_CHUNK_SIZE) {
      // Extract a chunk of proper size
      const chunkSize = Math.floor(pcmBufferRef.current.length / MIN_CHUNK_SIZE) * MIN_CHUNK_SIZE;
      const chunk = pcmBufferRef.current.slice(0, chunkSize);
      pcmBufferRef.current = pcmBufferRef.current.slice(chunkSize);

      // Add to queue
      audioQueueRef.current.push(chunk.buffer);
    }

    // Start playing if we have enough chunks (Jitter Buffer)
    // Wait for 3 chunks or if we have a lot of data to ensure smooth playback
    // This adds a small initial latency but prevents "nervous" stuttering
    const MIN_CHUNKS_TO_START = 3;
    if (!isPlayingRef.current && audioQueueRef.current.length >= MIN_CHUNKS_TO_START) {
      console.log('‚ñ∂Ô∏è Jitter buffer full, starting playback');
      playAudioQueue();
    } else if (!isPlayingRef.current && audioQueueRef.current.length > 0) {
      // Safety timeout: if we don't get 3 chunks within 500ms, start anyway
      // This prevents hanging if the response is very short (shorter than 3 chunks)
      if (!audioContextRef.current.bufferTimeout) {
        audioContextRef.current.bufferTimeout = setTimeout(() => {
          if (!isPlayingRef.current && audioQueueRef.current.length > 0) {
            console.log('‚ñ∂Ô∏è Jitter buffer timeout, starting playback early');
            playAudioQueue();
          }
          audioContextRef.current.bufferTimeout = null;
        }, 500);
      }
    }
  };

  const activeSourcesRef = useRef([]); // Track active audio sources for interruption

  const stopAgentSpeech = () => {
    // 1. Stop all currently playing audio sources
    activeSourcesRef.current.forEach(source => {
      try {
        source.stop();
      } catch (e) {
        // Ignore errors if already stopped
      }
    });
    activeSourcesRef.current = [];

    // 2. Clear audio queues
    audioQueueRef.current = [];
    pcmBufferRef.current = new Float32Array(0);

    // 3. Cancel any browser TTS
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel();
    }
    ttsQueueRef.current = [];

    // 4. Reset flags
    isPlayingRef.current = false;
    isSpeakingRef.current = false;

    // 5. Update UI
    if (isCallActiveRef.current) {
      setStatus('listening');
    }

    console.log('üõë Agent speech interrupted (Barge-in)');
  };

  const playAudioQueue = async () => {
    if (isPlayingRef.current || audioQueueRef.current.length === 0) {
      return;
    }

    isPlayingRef.current = true;
    setStatus('speaking');

    // CRITICAL for Echo Cancellation: Stop recognition while speaking
    // This prevents the agent from hearing itself on speakers
    if (recognitionRef.current) {
      try {
        recognitionRef.current.abort();
        console.log('üîá Recognition paused for playback');

        // Safety delay to ensure microphone is fully off before audio starts
        await new Promise(resolve => setTimeout(resolve, 200));
      } catch (e) {
        // Ignore errors
      }
    }

    try {
      // Audio context should already be initialized
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000,
        });
      }

      const audioContext = audioContextRef.current;

      // Resume audio context if suspended (browser autoplay policy)
      if (audioContext.state === 'suspended') {
        try {
          await audioContext.resume();
          console.log('‚úÖ Audio context resumed in playAudioQueue');
        } catch (error) {
          console.error('‚ùå Failed to resume audio context:', error);
        }
      }

      // Initialize nextStartTime if not set
      if (!audioContextRef.current.nextStartTime) {
        audioContextRef.current.nextStartTime = audioContext.currentTime;
      }

      // Process all available chunks
      const sources = [];

      while (audioQueueRef.current.length > 0) {
        const chunk = audioQueueRef.current.shift();

        // Convert PCM 16-bit, 16kHz to AudioBuffer
        const audioBuffer = createPCMBuffer(audioContext, chunk);
        if (!audioBuffer) {
          continue; // skip invalid chunks
        }

        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);

        // Schedule playback
        // If we fell behind real-time (underrun), jump to current time + small buffer
        let startTime = Math.max(audioContext.currentTime, audioContextRef.current.nextStartTime);

        // If we're resetting (gap > 0.1s), add a larger delay to allow network to catch up
        if (startTime === audioContext.currentTime) {
          startTime += 0.05; // 50ms safety buffer (Jitter tolerance)
        }

        source.start(startTime);

        // Track source for interruption
        activeSourcesRef.current.push(source);
        source.onended = () => {
          // Remove from active sources when done
          activeSourcesRef.current = activeSourcesRef.current.filter(s => s !== source);
        };

        sources.push({ source, endTime: startTime + audioBuffer.duration });

        // Update next start time
        audioContextRef.current.nextStartTime = startTime + audioBuffer.duration;
      }

      // Wait for playback to complete (keeps 'speaking' state active)
      if (sources.length > 0) {
        const lastEndTime = sources[sources.length - 1].endTime;
        const waitTime = (lastEndTime - audioContext.currentTime) * 1000;

        if (waitTime > 0) {
          // Minimal wait for low latency - just 10ms overhead
          await new Promise(resolve => setTimeout(resolve, waitTime + 10));
        }
      }

    } catch (error) {
      console.error('Error playing audio:', error);
    } finally {
      // Only proceed if we haven't been interrupted
      // If activeSourcesRef is empty but we were playing, means we were interrupted
      if (isPlayingRef.current) {
        isPlayingRef.current = false;

        // Check if more chunks arrived while playing
        // Lower threshold to flush remaining audio at end of sentence
        if (audioQueueRef.current.length > 0 || pcmBufferRef.current.length >= 512) {
          // Continue playing immediately
          setTimeout(() => playAudioQueue(), 0);
        } else {
          // No more chunks, update status
          // Note: Recognition is ALREADY running, so we don't need to restart it!
          // No more chunks, update status
          setStatus(isCallActive ? 'listening' : 'idle');

          // Restart recognition after speaking
          if (isCallActive) {
            restartRecognition();
          }
        }
      }
    }
  };

  const createPCMBuffer = (audioContext, arrayBuffer) => {
    // Guard against empty buffers
    if (!arrayBuffer || arrayBuffer.byteLength < 2) {
      return null;
    }

    // Convert PCM 16-bit, 16kHz to AudioBuffer
    const sampleRate = 16000;
    // 16-bit = 2 bytes per sample, so length is byteLength / 2
    const length = Math.floor(arrayBuffer.byteLength / 2);
    const buffer = audioContext.createBuffer(1, length, sampleRate);
    const data = buffer.getChannelData(0);
    const view = new DataView(arrayBuffer);

    for (let i = 0; i < length; i++) {
      const int16 = view.getInt16(i * 2, true); // Little-endian
      // Direct conversion to float [-1.0, 1.0] without any extra processing
      data[i] = int16 / 32768.0;
    }

    return buffer;
  };

  const startCall = async () => {
    try {
      setError(null);

      // Initialize audio context early to ensure it's ready for playback
      // This helps with browser autoplay policy - user interaction (clicking Start Call) allows audio
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000,
        });
        console.log('‚úÖ Audio context initialized');
      }

      // Resume audio context if suspended (browser autoplay policy)
      if (audioContextRef.current.state === 'suspended') {
        try {
          await audioContextRef.current.resume();
          console.log('‚úÖ Audio context resumed at call start');
        } catch (error) {
          console.error('‚ùå Failed to resume audio context:', error);
        }
      }

      // Create gain node early
      if (!audioContextRef.current.gainNode) {
        audioContextRef.current.gainNode = audioContextRef.current.createGain();
        audioContextRef.current.gainNode.gain.value = 1.0;
        audioContextRef.current.gainNode.connect(audioContextRef.current.destination);
        console.log('‚úÖ Gain node created');
      }

      // Connect WebSocket if not connected
      if (!isConnected || !wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
        connectWebSocket();

        // Wait for connection with timeout
        let attempts = 0;
        const maxAttempts = 20; // 2 seconds total

        while (attempts < maxAttempts) {
          if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
            setIsConnected(true);
            break;
          }
          await new Promise((resolve) => setTimeout(resolve, 100));
          attempts++;
        }

        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          const state = wsRef.current ? wsRef.current.readyState : 'null';
          const stateNames = { 0: 'CONNECTING', 1: 'OPEN', 2: 'CLOSING', 3: 'CLOSED' };
          console.error('WebSocket connection failed. State:', state, stateNames[state] || 'UNKNOWN');

          // Check if connection was closed
          if (state === 3 || state === 'CLOSED') {
            setError('Connection closed. Please check backend logs - Deepgram initialization may have failed.');
          } else {
            setError('Failed to connect to server. Please ensure the backend is running on port 8080.');
          }
          return;
        }
      }

      // Use Web Speech API (free) or Deepgram (backend)
      if (useWebSpeechAPI && 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        console.log('üé§ Using Web Speech API (FREE) for STT');

        // Initialize Web Speech API
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
          console.log('‚úÖ Web Speech API started');
          setStatus('listening');
        };

        // Turbo VAD: Track interim speech to detect silence faster than the API
        const silenceThreshold = 700; // ms of silence to consider "done"

        // Clear any previous interval
        if (silenceTimerRef.current) clearInterval(silenceTimerRef.current);

        silenceTimerRef.current = setInterval(() => {
          // Don't process VAD if we are speaking/playing audio
          if (!isCallActiveRef.current || !recognitionRef.current || isSpeakingRef.current || isPlayingRef.current) return;

          const now = Date.now();
          const timeSinceLastSpeech = now - lastSpeechTimeRef.current;
          const hasInterim = interimTranscriptRef.current && interimTranscriptRef.current.trim().length > 0;

          // If we have pending speech and enough silence has passed
          if (hasInterim && timeSinceLastSpeech > silenceThreshold) {
            console.log(`üöÄ Turbo VAD: Silence detected (${timeSinceLastSpeech}ms) - Sending transcript manually`);

            const textToSend = interimTranscriptRef.current.trim();

            // Send if valid
            if (textToSend && wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
              console.log('üìù Turbo VAD Sending:', textToSend);
              wsRef.current.send(JSON.stringify({
                type: 'transcript',
                text: textToSend
              }));

              // Clear buffer immediately to prevent double sending
              interimTranscriptRef.current = '';

              // Force restart recognition to clear internal API buffer
              // This prevents the API from sending a "final" event later with the same text
              try {
                recognitionRef.current.abort();
              } catch (e) { console.warn('Abort failed', e); }
            }
          }
        }, 100);

        recognition.onresult = (event) => {
          let finalTranscript = '';
          let interimTranscript = '';

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalTranscript += transcript + ' ';
            } else {
              interimTranscript += transcript;
            }
          }

          // Debug logs for tuning
          // console.log('Speech Event:', { final: finalTranscript, interim: interimTranscript });

          // Update refs for Turbo VAD
          if (interimTranscript.trim().length > 0) {
            lastSpeechTimeRef.current = Date.now();
            interimTranscriptRef.current = interimTranscript;
          } else if (finalTranscript.trim().length > 0) {
            // If we got a final, clear interim logic so we don't double send
            interimTranscriptRef.current = '';
            lastSpeechTimeRef.current = Date.now();
          }

          // BARGE-IN LOGIC: If user speaks while agent is speaking, stop the agent!
          if (isSpeakingRef.current && (finalTranscript || interimTranscript.length > 2)) {
            console.log('üó£Ô∏è User starts speaking - Interrupting agent (Barge-in)');
            stopAgentSpeech();
            // Clear interim buffer on barge-in to avoid processing old speech
            interimTranscriptRef.current = '';

            // Optionally send stop signal to backend to cancel generation
            if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
              wsRef.current.send(JSON.stringify({ type: 'stop_generation' }));
            }
          }

          // Send final transcript to backend
          // Note: Turbo VAD might have already sent this if it was slow.
          // But if the API is fast enough, we send it here.
          if (finalTranscript.trim()) {
            console.log('üìù Final transcript (API):', finalTranscript.trim());

            if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
              wsRef.current.send(JSON.stringify({
                type: 'transcript',
                text: finalTranscript.trim()
              }));
            }
            // Clear interim ref since we just sent final
            interimTranscriptRef.current = '';
          }
        };

        recognition.onerror = (event) => {
          // 'aborted' is expected when we Turbo VAD abort or manually stop - ignore it
          if (event.error === 'aborted') {
            return;
          }

          console.error('‚ùå Web Speech API error:', event.error);

          // CRITICAL: Always try to restart for persistent listening
          // BUT only if we are not currently speaking (to prevent echo)
          if (isCallActiveRef.current && !isSpeakingRef.current && !isPlayingRef.current) {
            const restartDelay = event.error === 'no-speech' ? 50 : 200; // Even faster restart
            setTimeout(() => {
              if (isCallActiveRef.current && recognitionRef.current && !isSpeakingRef.current && !isPlayingRef.current) {
                // Don't restart if already started (handled by catch block in restartRecognition)
                restartRecognition();
              }
            }, restartDelay);
          }
        };

        recognition.onend = () => {
          console.log('‚ö†Ô∏è Web Speech API ended');
          // For persistent listening, ALWAYS restart if call is active
          // BUT ONLY RESTART IF NOT SPEAKING (to prevent self-listening/echo)
          if (isCallActiveRef.current && !isSpeakingRef.current && !isPlayingRef.current) {
            console.log('üîÑ Restarting Web Speech API (persistent listening)...');
            setTimeout(() => {
              if (isCallActiveRef.current && recognitionRef.current && !isSpeakingRef.current && !isPlayingRef.current) {
                restartRecognition();
              }
            }, 100);
          } else {
            console.log('‚è∏Ô∏è Web Speech API paused (Agent speaking or intentional stop)');
          }
        };

        recognitionRef.current = recognition;
        recognition.start();

        // Store stream reference for cleanup
        mediaRecorderRef.current = { recognition, stream: null };
        isCallActiveRef.current = true;
        setIsCallActive(true);
        console.log('‚úÖ Call started with Web Speech API');
        return;
      }

      // Fallback to Deepgram (backend STT)
      console.log('üé§ Using Deepgram (backend) for STT');

      // Request microphone access
      console.log('üé§ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      console.log('‚úÖ Microphone access granted');
      console.log('üìä Audio tracks:', stream.getAudioTracks().length);
      stream.getAudioTracks().forEach((track, index) => {
        console.log(`üìä Track ${index} settings:`, track.getSettings());
        console.log(`üìä Track ${index} enabled:`, track.enabled, 'readyState:', track.readyState);
      });

      // Initialize AudioContext for processing
      const audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000,
      });
      audioContextRef.current = audioContext;
      console.log('‚úÖ AudioContext created, sample rate:', audioContext.sampleRate);

      const source = audioContext.createMediaStreamSource(stream);
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      console.log('‚úÖ Audio processor created, buffer size:', processor.bufferSize);

      let audioChunkCount = 0;
      let lastLogTime = Date.now();

      processor.onaudioprocess = (e) => {
        // Log first few callbacks to verify it's firing
        if (audioChunkCount < 3) {
          console.log(`üîä Audio callback fired #${audioChunkCount + 1}`);
        }

        // Use ref instead of state to get current value
        if (!isCallActiveRef.current) {
          if (audioChunkCount === 0) {
            console.log('‚ö†Ô∏è Call not active - isCallActiveRef:', isCallActiveRef.current);
          }
          return;
        }

        if (!wsRef.current) {
          if (audioChunkCount === 0) {
            console.log('‚ö†Ô∏è WebSocket not available');
          }
          return;
        }

        if (wsRef.current.readyState !== WebSocket.OPEN) {
          if (audioChunkCount === 0) {
            console.log('‚ö†Ô∏è WebSocket not open, state:', wsRef.current.readyState);
          }
          return;
        }

        const inputData = e.inputBuffer.getChannelData(0);

        // Check if we have actual audio data
        if (!inputData || inputData.length === 0) {
          if (audioChunkCount === 0) {
            console.warn('‚ö†Ô∏è No input data in audio buffer');
          }
          return;
        }

        // Log first chunk details
        if (audioChunkCount === 0) {
          console.log(`üìä First audio chunk - samples: ${inputData.length}, expected bytes: ${inputData.length * 2}`);
        }

        // Convert Float32Array to PCM 16-bit
        const pcmData = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          // Clamp and convert to 16-bit integer
          const s = Math.max(-1, Math.min(1, inputData[i]));
          pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        // Send PCM audio to server
        try {
          // Use the buffer directly from Int16Array - it's already in the correct format
          // Int16Array.buffer is an ArrayBuffer with the correct byte length
          const buffer = pcmData.buffer;

          // Verify buffer size (should be samples * 2 bytes)
          const expectedSize = inputData.length * 2;
          if (buffer.byteLength !== expectedSize) {
            console.warn(`‚ö†Ô∏è Buffer size mismatch: expected ${expectedSize}, got ${buffer.byteLength}`);
          }

          // Always send if buffer has data (remove the 100 byte minimum check)
          if (buffer.byteLength > 0) {
            // Send the buffer directly
            wsRef.current.send(buffer);
            audioChunkCount++;

            const now = Date.now();
            if (audioChunkCount === 1 || audioChunkCount % 50 === 0 || (now - lastLogTime) > 2000) {
              console.log(`üì§ Sent ${audioChunkCount} audio chunks, size: ${buffer.byteLength} bytes, samples: ${inputData.length}`);
              lastLogTime = now;
            }
          } else {
            if (audioChunkCount === 0) {
              console.warn(`‚ö†Ô∏è Skipping send - empty buffer (expected ~${expectedSize} bytes)`);
            }
          }
        } catch (error) {
          console.error('‚ùå Error sending audio:', error);
          console.error('Error details:', error.message);
          console.error('Error stack:', error.stack);
        }
      };

      source.connect(processor);
      // Don't connect processor to destination to avoid feedback
      // We only want to send audio to WebSocket, not play it back

      mediaRecorderRef.current = { stream, processor, source };
      isCallActiveRef.current = true; // Set ref immediately
      setIsCallActive(true);
      setStatus('listening');
      console.log('‚úÖ Call started - audio processing active');
      console.log('üìä Audio context sample rate:', audioContext.sampleRate);
      console.log('üìä Processor buffer size:', processor.bufferSize);
    } catch (error) {
      console.error('Error starting call:', error);
      setError(error.message || 'Failed to start call. Please check microphone permissions.');
    }
  };

  const stopCall = () => {
    isCallActiveRef.current = false; // Set ref immediately to stop audio processing
    setIsCallActive(false);
    setStatus('idle');
    audioQueueRef.current = [];
    isPlayingRef.current = false;
    console.log('üõë Call stopped - audio processing disabled');

    // Stop all audio immediately
    stopAgentSpeech();

    // Stop Web Speech API if active
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
        console.log('üõë Web Speech API stopped');
      } catch (e) {
        console.error('Error stopping Web Speech API:', e);
      }
    }

    // Stop TTS and clear queue
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel();
      ttsQueueRef.current = [];
      isSpeakingRef.current = false;
      console.log('üõë TTS stopped and queue cleared');
    }

    // Stop media stream (for Deepgram mode)
    if (mediaRecorderRef.current) {
      if (mediaRecorderRef.current.stream) {
        mediaRecorderRef.current.stream.getTracks().forEach((track) => track.stop());
      }
      if (mediaRecorderRef.current.processor) {
        mediaRecorderRef.current.processor.disconnect();
      }
      if (mediaRecorderRef.current.source) {
        mediaRecorderRef.current.source.disconnect();
      }
      mediaRecorderRef.current = null;
    }

    // Close WebSocket gracefully
    if (wsRef.current) {
      try {
        // Send close message and let server handle the close
        if (wsRef.current.readyState === WebSocket.OPEN) {
          wsRef.current.send(JSON.stringify({ type: 'close' }));
          // Don't close immediately - let server close it with proper handshake
          // Set a timeout to force close if server doesn't respond
          setTimeout(() => {
            if (wsRef.current && wsRef.current.readyState !== WebSocket.CLOSED) {
              console.log('Server did not close connection, closing client side');
              wsRef.current.close(1000, 'Call ended by user');
            }
            wsRef.current = null;
          }, 500);
        } else {
          // Already closed or closing
          wsRef.current = null;
        }
      } catch (error) {
        console.error('Error closing WebSocket:', error);
        if (wsRef.current) {
          try {
            wsRef.current.close(1000, 'Call ended');
          } catch (e) {
            // Ignore errors if already closed
          }
          wsRef.current = null;
        }
      }
    }

    setIsConnected(false);
  };

  return (
    <div className="app">
      <div className="container">
        <div className="header">
          <h1>üéôÔ∏è Voice AI Agent</h1>
          <p className="subtitle">Talk to Riya - Your Admissions Assistant</p>
        </div>

        <div className="status-card">
          <div className="status-indicator">
            <div className={`pulse ${status}`}></div>
            <span className="status-text">
              {status === 'idle' && 'Ready to start'}
              {status === 'listening' && 'üéß Listening...'}
              {status === 'processing' && '‚öôÔ∏è Processing...'}
              {status === 'speaking' && 'üîä Agent speaking...'}
            </span>
          </div>

          {error && (
            <div className="error-message">
              ‚ö†Ô∏è {error}
            </div>
          )}

          <div className="connection-status">
            <span className={`status-dot ${isConnected ? 'connected' : 'disconnected'}`}></span>
            {isConnected ? 'Connected' : 'Disconnected'}
          </div>
        </div>

        <div className="controls">
          {!isCallActive ? (
            <button
              className="btn btn-start"
              onClick={startCall}
              disabled={false}
            >
              <span className="btn-icon">üìû</span>
              Start Call
            </button>
          ) : (
            <button className="btn btn-end" onClick={stopCall}>
              <span className="btn-icon">üì¥</span>
              End Call
            </button>
          )}
        </div>

        <div className="info">
          <p>üí° <strong>How it works:</strong></p>
          <ul>
            <li>Click "Start Call" to begin</li>
            <li>Speak naturally - the agent will listen</li>
            <li>Wait for Riya to respond</li>
            <li>Click "End Call" when finished</li>
          </ul>
        </div>
      </div>
    </div>
  );
}

export default App;

